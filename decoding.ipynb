{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9rCTpR6Z9KyZ"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import shutil\n",
        "from functools import partial\n",
        "from loguru import logger\n",
        "\n",
        "# data management\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# ML\n",
        "import torch\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# custom modules\n",
        "from src.learning import models\n",
        "from src.learning.estimators import PytorchEstimator\n",
        "from src.learning.losses import multinomial_cross_entropy\n",
        "from src.learning.metrics import mean_auc, recall_n\n",
        "from src.utils.dataframe import dumb_tagger\n",
        "from src.utils.file import get_json_files_in_dir, mkdir\n",
        "from src.tools import (\n",
        "    mask_rows,\n",
        "    yes_or_no,\n",
        "    gridsearch_complexity,\n",
        "    highly_corr_cols_np,\n",
        "    one_compact_line,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decoding_experiment(configuration=\"spec_template.json\",\n",
        "                        mode=\"train\",\n",
        "                        folder=\"\",\n",
        "                        results_file=\"\",\n",
        "                        used_gpu=0,\n",
        "                        n_jobs=1,\n",
        "                        verbose=False,\n",
        "                        plot_training=False,\n",
        "                        force=False):\n",
        "\n",
        "    ID = configuration[:-5].split(\"/\")[-1]\n",
        "\n",
        "    with open(configuration, encoding='utf-8') as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    # Path where to save transformed data, model and results\n",
        "    path = folder + ID + \"/\"\n",
        "    per_label_results_file = path + \"per_label_results.csv\"\n",
        "\n",
        "    # Control whether the experiment was already run\n",
        "    # and results mught be overwritten\n",
        "    should_ask_question_for_experiment = (\n",
        "        (not force)\n",
        "        &\n",
        "        ((not mkdir(path)) & (mode == \"train\"))\n",
        "    )\n",
        "\n",
        "    model_classes = {\n",
        "        \"sk_logreg\": LogisticRegression,\n",
        "        \"sk_logreg_cv\": LogisticRegressionCV,\n",
        "        \"sk_svm\": SVC,\n",
        "        \"sk_lda\": LinearDiscriminantAnalysis,\n",
        "    }\n",
        "\n",
        "    loss_functions = {\n",
        "        \"logreg\": torch.nn.BCEWithLogitsLoss(reduction='none'),\n",
        "        \"multinomial\": multinomial_cross_entropy,\n",
        "    }\n",
        "\n",
        "    # Grid to explore\n",
        "    param_grid = config[\"grid_params\"]\n",
        "    exploratory_comp = gridsearch_complexity(param_grid)\n",
        "\n",
        "    # Initial values for model instanciation\n",
        "    param_ini = {k: v[0] for (k, v) in param_grid.items()}\n",
        "\n",
        "    # Metadata loading\n",
        "    meta = (\n",
        "        pd.read_csv(config[\"data\"].get(\"meta_file\"), low_memory=False, index_col=0)\n",
        "        .loc[lambda df: df.kept]\n",
        "    )\n",
        "\n",
        "    # Labels vocabulary loading\n",
        "    with open(config[\"data\"][\"concepts_file\"], encoding='utf-8') as f:\n",
        "        concept_names = [line.rstrip('\\n') for line in f]\n",
        "        concept_names = sorted([concept_name.strip().lower()\n",
        "                         for concept_name in concept_names])\n",
        "\n",
        "    # Features loading\n",
        "    with open(config[\"data\"].get(\"features_file\"), 'rb') as f:\n",
        "        X = pickle.load(f)\n",
        "\n",
        "    # Samples' labels loading\n",
        "    labels = pd.read_csv(config[\"data\"][\"labels_file\"], low_memory=False, index_col=0)\n",
        "\n",
        "    # Only keep samples (fMRIs metadata and their embeddings) with labels\n",
        "    mask_labelled = ~labels.iloc[:, 0].isna()\n",
        "    meta, X, labels = mask_rows(mask_labelled, meta, X, labels)\n",
        "\n",
        "    # Target as a one-hot encoding of labels\n",
        "    Y = dumb_tagger(labels,\n",
        "                    split_regex=r\",\\s*\",\n",
        "                    vocab=concept_names,\n",
        "                    label_col=None)\n",
        "\n",
        "    # Extract vocabulary of labels present in the dataset\n",
        "    vocab_orig = np.array(Y.columns)\n",
        "\n",
        "    # Convert Y to np.array of int\n",
        "    Y = Y.values * 1\n",
        "\n",
        "    # In case the labels did not come from the proper vocabulary,\n",
        "    #   remove the fmris without any label\n",
        "    mask_label_checked = (Y.sum(axis=1) != 0)\n",
        "    meta, X, Y = mask_rows(mask_label_checked, meta, X, Y)\n",
        "    \n",
        "    # Remove maps from blacklist if present\n",
        "    if config[\"data\"].get(\"blacklist\"):\n",
        "        mask_not_blacklisted = np.full(len(meta), True)\n",
        "        blacklist = config[\"data\"].get(\"blacklist\")\n",
        "        for blacklist_key in blacklist:\n",
        "            mask_not_blacklisted = (\n",
        "                mask_not_blacklisted\n",
        "                &\n",
        "                ~meta[blacklist_key].isin(blacklist[blacklist_key])\n",
        "            )\n",
        "        meta, X, Y = mask_rows(mask_not_blacklisted, meta, X, Y)\n",
        "\n",
        "    # Filtering labels with too few instances in train\n",
        "    mask_test = (meta[\"collection_id\"].isin(config[\"evaluation\"][\"test_IDs\"]))\n",
        "    colmask_lab_in_train = (Y[~mask_test].sum(axis=0)\n",
        "                            >= config[\"labels\"][\"min_train\"])\n",
        "\n",
        "    number_of_rare_labels = len(vocab_orig) - int(colmask_lab_in_train.sum())\n",
        "\n",
        "    # updating X and Y\n",
        "    Y = Y[:, colmask_lab_in_train]\n",
        "    mask_lab_in_train = (np.sum(Y, axis=1) != 0)\n",
        "    meta, X, Y = mask_rows(mask_lab_in_train, meta, X, Y)\n",
        "\n",
        "    # updating vocab mask\n",
        "    vocab_current = vocab_orig[colmask_lab_in_train]\n",
        "\n",
        "    # Remove almost fully correlated columns\n",
        "    labels_low_corr_indices = highly_corr_cols_np(Y,\n",
        "                                                  vocab_current,\n",
        "                                                  0.95,\n",
        "                                                  True)\n",
        "\n",
        "    number_of_too_correlated_labels = Y.shape[1] - len(labels_low_corr_indices)\n",
        "\n",
        "    Y = Y[:, labels_low_corr_indices]\n",
        "    vocab_current = vocab_current[labels_low_corr_indices]\n",
        "\n",
        "    # Update of data and testset mask after highly correlated labels removal\n",
        "    mask_has_low_corr_lab = (np.sum(Y, axis=1) != 0)\n",
        "    meta, X, Y = mask_rows(mask_has_low_corr_lab, meta, X, Y)\n",
        "    mask_test = meta[\"collection_id\"].isin(config[\"evaluation\"][\"test_IDs\"])\n",
        "    \n",
        "    # save original version of labels to predict before labels inference\n",
        "    Y_orig = Y.copy()\n",
        "    \n",
        "\n",
        "    # Concept values transformations\n",
        "    if (config[\"labels\"].get(\"transformation\") == \"none\"\n",
        "            or config[\"labels\"].get(\"transformation\") is None):\n",
        "        pass\n",
        "    elif config[\"labels\"].get(\"transformation\") == \"thresholding\":\n",
        "        Y = (Y >= config[\"labels\"][\"threshold\"]) * 1\n",
        "    elif config[\"labels\"].get(\"transformation\") == \"normalization\":\n",
        "        Y = Y / Y.sum(axis=1, keepdims=True)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported transformation of concept values\")\n",
        "\n",
        "    # ------------------------------\n",
        "    # --- FEATURES PREPROCESSING ---\n",
        "    # ------------------------------\n",
        "    # Thresholding\n",
        "    if config[\"data\"].get(\"positive_values\"):\n",
        "        X[X < 0] = 0\n",
        "\n",
        "    # Train/valid split\n",
        "    X_train, Y_train_orig, Y_train = mask_rows(~mask_test, X, Y_orig, Y)\n",
        "    X_test, Y_test_orig, Y_test = mask_rows(mask_test, X, Y_orig, Y)\n",
        "    indices_train = list(meta[~mask_test].index)\n",
        "    indices_test = list(meta[mask_test].index)\n",
        "\n",
        "    # Save original difumo values before normalization\n",
        "    for variable_name in [\"X_train\", \"X_test\"]:\n",
        "        output_path_for_variable = f\"{path}{variable_name}_raw.p\"\n",
        "        with open(output_path_for_variable, \"wb\") as f:\n",
        "            # eval(name) retrieves the variable value which has this name\n",
        "            pickle.dump(eval(variable_name), f)\n",
        "\n",
        "    # Scaling over features or samples based on train dataset\n",
        "    if config[\"data\"].get(\"scaling\") == \"features\":\n",
        "        scaler = preprocessing.StandardScaler()\n",
        "        scaler.fit(X_train)\n",
        "        X_train = scaler.transform(X_train)\n",
        "        X_test = scaler.transform(X_test)\n",
        "    elif config[\"data\"].get(\"scaling\") == \"samples\":\n",
        "        preprocessing_on_samples = partial(preprocessing.scale, with_mean=True, with_std=True, axis=1)\n",
        "        X_train = preprocessing_on_samples(X_train)\n",
        "        X_test = preprocessing_on_samples(X_test)\n",
        "    elif config[\"data\"].get(\"scaling\") == \"max\":\n",
        "        X_train_max = X_train.max()\n",
        "        X_train = X_train / X_train_max\n",
        "        X_test = X_test / X_train_max\n",
        "\n",
        "    # ------------------------\n",
        "    # --- SAMPLING WEIGHTS ---\n",
        "    # ------------------------\n",
        "    # Groups definition (for CV splits, loss reweighting and sampling)\n",
        "    meta = (\n",
        "        meta.assign(\n",
        "            group=lambda df: (df[\"collection_id\"].astype(str) + \" \" + df[\"cognitive_paradigm_cogatlas\"].astype(str))\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Reweighting samples if required\n",
        "    weighted_samples = (\n",
        "        config[\"torch_params\"].get(\"group_power\")\n",
        "        and config[\"torch_params\"][\"group_power\"] < 1.0\n",
        "    )\n",
        "    if weighted_samples:\n",
        "        group_size = meta.groupby(\"group\")[\"kept\"].count()\n",
        "        group_size.name = \"group_size\"\n",
        "\n",
        "        # sampling weights\n",
        "        group_weights = meta.join(group_size, on=\"group\")[\"group_size\"]\n",
        "        group_weights = (group_weights ** config[\"torch_params\"][\"group_power\"] / group_weights)\n",
        "        sample_weights = group_weights.values.reshape((-1, 1))\n",
        "        sample_weights_train = sample_weights[~mask_test]\n",
        "        X_train = np.hstack((sample_weights_train, X_train))\n",
        "\n",
        "    for variable_name in [\"X_train\", \"Y_train\", \"Y_train_orig\", \"X_test\", \"Y_test\", \"Y_test_orig\", \"indices_train\", \"indices_test\"]:\n",
        "        output_path_for_variable = f\"{path}{variable_name}.p\"\n",
        "        with open(output_path_for_variable, \"wb\") as f:\n",
        "            pickle.dump(eval(variable_name), f)\n",
        "\n",
        "    pd.DataFrame(vocab_orig).to_csv(f\"{path}vocab_orig.csv\")\n",
        "    pd.DataFrame(vocab_current).to_csv(f\"{path}vocab.csv\")\n",
        "\n",
        "    if config.get(\"estimator_type\") == \"sklearn\":\n",
        "        estimator_type = \"sklearn\"\n",
        "        clf_template = model_classes[config[\"model_name\"]]\n",
        "    else:\n",
        "        estimator_type = \"pytorch\"\n",
        "        clf_template = PytorchEstimator(\n",
        "            gpu=(used_gpu > -1),\n",
        "            used_gpu=used_gpu,\n",
        "            model_class=getattr(models, config[\"model_name\"]),\n",
        "            loss_func=loss_functions[config[\"loss\"][\"loss_func_name\"]],\n",
        "            epochs=config[\"torch_params\"][\"epochs\"],\n",
        "            batch_size=config[\"torch_params\"][\"batch_size\"],\n",
        "            adam=config[\"torch_params\"][\"Adam\"],\n",
        "            verbose=plot_training*2,\n",
        "            **param_ini\n",
        "        )\n",
        "\n",
        "    clf, clf_grid = None, None\n",
        "    if exploratory_comp == 1:\n",
        "        logger.info(\"Single model training...\")\n",
        "        if estimator_type == \"sklearn\":\n",
        "            clfs = {}\n",
        "            for i, concept in enumerate(vocab_current):\n",
        "                logger.info(f\"Training for {concept}\")\n",
        "                clf = clf_template(**param_ini)\n",
        "                clf.fit(X_train, Y_train[:, i])\n",
        "                clfs[concept] = clf\n",
        "        else:\n",
        "            clf = clf_template\n",
        "            clf.fit(X_train, Y_train,\n",
        "                    weighted_samples=weighted_samples,\n",
        "                    n_jobs=n_jobs)\n",
        "\n",
        "    else:\n",
        "        assert estimator_type == \"pytorch\", \"grid search not implemented yet\" \\\n",
        "                                            \" for sklearn estimators\"\n",
        "        if (\n",
        "                not config[\"torch_params\"].get(\"search\")\n",
        "                or\n",
        "                config[\"torch_params\"][\"search\"] == -1\n",
        "                or\n",
        "                config[\"torch_params\"][\"search\"] > exploratory_comp\n",
        "        ):\n",
        "            if verbose:\n",
        "                print(\"  > Cross validation with\",\n",
        "                      config[\"torch_params\"][\"splits\"],\n",
        "                      \"splits,\",\n",
        "                      exploratory_comp * config[\"torch_params\"][\"splits\"] + 1,\n",
        "                      \"models fitted\")\n",
        "            clf_grid = GridSearchCV(clf_template, param_grid,\n",
        "                                    cv=config[\"torch_params\"][\"splits\"],\n",
        "                                    iid=False, n_jobs=n_jobs,\n",
        "                                    verbose=verbose*1)\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(\n",
        "                    \"  > Cross validation with\",\n",
        "                    config[\"torch_params\"][\"splits\"],\n",
        "                    \"splits,\",\n",
        "                    config[\"torch_params\"][\"search\"]\n",
        "                    * config[\"torch_params\"][\"splits\"] + 1,\n",
        "                    \"models fitted\"\n",
        "                )\n",
        "            clf_grid = RandomizedSearchCV(\n",
        "                clf_template, param_grid,\n",
        "                cv=config[\"torch_params\"][\"splits\"],\n",
        "                n_iter=config[\"torch_params\"][\"search\"],\n",
        "                iid=False, n_jobs=n_jobs,\n",
        "                verbose=verbose * 1\n",
        "            )\n",
        "        clf_grid.fit(X_train, Y_train,\n",
        "                     meta.loc[~mask_test][\"group\"].values,\n",
        "                     weighted_samples=weighted_samples)\n",
        "\n",
        "        # Grid search detailed results backup\n",
        "        pd.DataFrame(clf_grid.cv_results_).to_csv(f\"{path}per_param_results.csv\")\n",
        "\n",
        "        clf = clf_grid.best_estimator_\n",
        "\n",
        "        print(\"  > Best params:\", str(clf_grid.best_params_))\n",
        "        print(\"  > Best score:\", str(clf_grid.best_score_))\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "    # Model backup\n",
        "    if estimator_type == \"sklearn\":\n",
        "        with open(path + \"clf.p\", 'wb') as f:\n",
        "            pickle.dump(clfs, f)\n",
        "    else:\n",
        "        with open(path + \"clf.p\", 'wb') as f:\n",
        "            pickle.dump(clf, f)\n",
        "        torch.save(clf.model, path + \"clf.pt\")\n",
        "\n",
        "    N = config[\"evaluation\"].get(\"recall@N\")\n",
        "    if N is None:\n",
        "        N = Y_train.shape[1] // 10\n",
        "\n",
        "    # Dumb predictions based on labels ranking in training set\n",
        "    labels_rank_train = np.sum(Y_train, axis=0)\n",
        "    Y_train_pred_dumb = np.tile(labels_rank_train, [len(Y_train), 1])\n",
        "    Y_test_pred_dumb = np.tile(labels_rank_train, [len(Y_test), 1])\n",
        "\n",
        "    # Remove weights from features for final prediction (1st col)\n",
        "    if weighted_samples:\n",
        "        X_train = X_train[:, 1:]\n",
        "\n",
        "    # Predict labels on TEST set with best classifier\n",
        "    if estimator_type == \"sklearn\":\n",
        "        Y_train_pred = np.zeros((len(X_train), len(vocab_current)))\n",
        "        Y_test_pred = np.zeros((len(X_test), len(vocab_current)))\n",
        "        for i, concept in enumerate(vocab_current):\n",
        "            Y_train_pred[:, i] = clfs[concept].predict(X_train)\n",
        "            Y_test_pred[:, i] = clfs[concept].predict(X_test)\n",
        "\n",
        "    else:\n",
        "        Y_train_pred = clf.predict(X_train)\n",
        "        Y_test_pred = clf.predict(X_test)\n",
        "\n",
        "    # Compute recalls for dumb and trained classifiers\n",
        "    weighted_recall_n = partial(recall_n, n=N, reduce_mean=True)\n",
        "    recall_train_dumb = weighted_recall_n(Y_train_pred_dumb, Y_train_orig)\n",
        "    recall_test_dumb = weighted_recall_n(Y_test_pred_dumb, Y_test_orig)\n",
        "    recall_train = weighted_recall_n(Y_train_pred, Y_train_orig)\n",
        "    recall_test = weighted_recall_n(Y_test_pred, Y_test_orig)\n",
        "\n",
        "    # Compute AUC for trained classifier\n",
        "    auc = mean_auc(Y_test_pred, Y_test_orig)\n",
        "\n",
        "    # Print and save recap on experiment\n",
        "    desc = \"Experiment: \" + ID\n",
        "    desc += \"\\n  Trained on: \" + str(datetime.datetime.now())\n",
        "    desc += \"\\n  Scaling: \" + str(config[\"data\"].get(\"scaling\"))\n",
        "    desc += (\"\\n  Positive values only: \"\n",
        "             + str(config[\"data\"].get(\"positive_values\")))\n",
        "    desc += (\"\\n  Group reweighting power: \"\n",
        "             + str(config[\"torch_params\"].get(\"group_power\")))\n",
        "    desc += (\"\\n  Concept similarity transformation: \"\n",
        "             + config[\"labels\"].get(\"transformation\"))\n",
        "    if estimator_type == \"sklearn\":\n",
        "        desc += \"\\n  Model: \" + config[\"model_name\"]\n",
        "    else:\n",
        "        desc += \"\\n  Model: \" + str(clf.model)\n",
        "    if exploratory_comp == 1:\n",
        "        desc += \"\\n  Parameters: \" + str(param_grid)\n",
        "        best_params = \"NA\"\n",
        "    else:\n",
        "        desc += \"\\n  Explored parameters: \" + str(param_grid)\n",
        "        desc += \"\\n  Best parameters: \" + str(clf_grid.best_params_)\n",
        "        best_params = str(clf_grid.best_params_)\n",
        "\n",
        "    # Append metrics to comparison file\n",
        "    results_df = pd.DataFrame(\n",
        "        data=[[\n",
        "            ID,\n",
        "            config[\"description\"],\n",
        "            str(datetime.datetime.now()),\n",
        "            config[\"data\"].get(\"scaling\"),\n",
        "            config[\"data\"].get(\"positive_values\"),\n",
        "            config[\"torch_params\"].get(\"group_power\"),\n",
        "            config[\"labels\"].get(\"transformation\"),\n",
        "            config[\"labels\"].get(\"threshold\"),\n",
        "            (config[\"model_name\"] if estimator_type == \"sklearn\"\n",
        "             else one_compact_line(clf.model)),\n",
        "            str(param_grid),\n",
        "            best_params,\n",
        "            N,\n",
        "            recall_train,\n",
        "            recall_test,\n",
        "            auc\n",
        "        ]],\n",
        "        columns=[\n",
        "            \"experiment\",\n",
        "            \"description\",\n",
        "            \"trained_on\",\n",
        "            \"scaling\",\n",
        "            \"positive_part\",\n",
        "            \"collection_regul\",\n",
        "            \"label_transformation\",\n",
        "            \"threshold\",\n",
        "            \"model\",\n",
        "            \"explored_params\",\n",
        "            \"best_params\",\n",
        "            \"N\",\n",
        "            \"recall_N_TRAIN\",\n",
        "            \"recall_N_TEST\",\n",
        "            \"AUC_TEST\"\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    if os.path.isfile(results_file):\n",
        "        previous_results = pd.read_csv(results_file, index_col=0)\n",
        "        full_results = previous_results.append(results_df)\n",
        "        full_results.index = list(range(len(full_results)))\n",
        "        full_results.to_csv(results_file, header=True)\n",
        "    else:\n",
        "        results_df.to_csv(results_file, header=True)\n",
        "\n",
        "    # Save per-label metrics\n",
        "    size_train = len(X_train)\n",
        "    size_test = len(X_test)\n",
        "    n_labels = len(vocab_current)\n",
        "    labels_in_test = pd.DataFrame([Y_test_orig.sum(axis=0)],\n",
        "                                  columns=vocab_current)\n",
        "    labels_in_train = pd.DataFrame([Y_train_orig.sum(axis=0)],\n",
        "                                   columns=vocab_current)\n",
        "\n",
        "    results = pd.DataFrame(\n",
        "        columns=[\"ratio TRAIN\",\n",
        "                 \"recall@10 TRAIN\",\n",
        "                 \"ratio TEST\",\n",
        "                 \"recall@10 TEST\",\n",
        "                 \"AUC TEST\"],\n",
        "        index=labels_in_test.columns[labels_in_test.values[0] > 0]\n",
        "    )\n",
        "\n",
        "    for i, label in enumerate(vocab_current):\n",
        "        if labels_in_test[label].values:\n",
        "            mask_label = np.zeros(n_labels)\n",
        "            mask_label[i] = 1\n",
        "            mask_samples_train = Y_train_orig[:, i] > 0\n",
        "            mask_samples_test = Y_test_orig[:, i] > 0\n",
        "            if labels_in_train[label].values:\n",
        "\n",
        "                results.loc[label] = [\n",
        "                    labels_in_train[label].values[0] / size_train,\n",
        "                    recall_n(Y_train_pred[mask_samples_train],\n",
        "                             Y_train_orig[mask_samples_train] * mask_label,\n",
        "                             n=10,\n",
        "                             reduce_mean=True),\n",
        "                    labels_in_test[label].values[0] / size_test,\n",
        "                    recall_n(Y_test_pred[mask_samples_test],\n",
        "                             Y_test_orig[mask_samples_test] * mask_label,\n",
        "                             n=10,\n",
        "                             reduce_mean=True),\n",
        "                    roc_auc_score(Y_test_orig[:, i], Y_test_pred[:, i])\n",
        "                    if (Y_test_orig[:, i].sum()) and (0 in Y_test_orig[:, i])\n",
        "                    else np.nan\n",
        "                ]\n",
        "            else:\n",
        "                results.loc[label] = [\n",
        "                    0,\n",
        "                    np.nan,\n",
        "                    labels_in_test[label].values[0] / size_test,\n",
        "                    recall_n(Y_test_pred[mask_samples_test],\n",
        "                             Y_test_orig[mask_samples_test] * mask_label,\n",
        "                             n=10,\n",
        "                             reduce_mean=True)\n",
        "                ]\n",
        "\n",
        "    results.sort_values(by=['recall@10 TEST'], ascending=False, inplace=True)\n",
        "    results.to_csv(per_label_results_file, header=True)\n",
        "\n",
        "    return clf\n"
      ],
      "metadata": {
        "id": "sdDUzkBW_MOI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configurations = get_json_files_in_dir(\"config/decoding_config.json\")\n",
        "\n",
        "for conf in configurations:\n",
        "    decoding_experiment(\n",
        "        configuration=conf,\n",
        "        mode=\"train\",\n",
        "        folder=\"cache\",\n",
        "        results_file=\"output\",\n",
        "        used_gpu=1,\n",
        "        n_jobs=1,\n",
        "        verbose=False,\n",
        "        force=False\n",
        "    )"
      ],
      "metadata": {
        "id": "s9iv9T_i_T3M"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z3COUi6jAeVZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}